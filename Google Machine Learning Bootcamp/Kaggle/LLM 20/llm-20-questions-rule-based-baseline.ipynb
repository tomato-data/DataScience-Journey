{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM 20 Questions Rule Based Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Set accelerator to GPU T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-24T03:36:29.569877Z",
     "iopub.status.busy": "2024-07-24T03:36:29.569513Z",
     "iopub.status.idle": "2024-07-24T03:37:16.609692Z",
     "shell.execute_reply": "2024-07-24T03:37:16.608675Z",
     "shell.execute_reply.started": "2024-07-24T03:36:29.569848Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p /kaggle/working/submission\n",
    "mkdir -p /tmp/model\n",
    "pip install -q bitsandbytes accelerate\n",
    "pip install -qU transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model\n",
    "\n",
    "### HuggingFace Login\n",
    "\n",
    "Add HugginFace access token to secrets. You can find it in `Add-ons -> secrets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:37:16.612002Z",
     "iopub.status.busy": "2024-07-24T03:37:16.611693Z",
     "iopub.status.idle": "2024-07-24T03:37:16.730352Z",
     "shell.execute_reply": "2024-07-24T03:37:16.729307Z",
     "shell.execute_reply.started": "2024-07-24T03:37:16.611976Z"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "\n",
    "HF_TOKEN: str | None  = None\n",
    "\n",
    "try:\n",
    "    HF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Model\n",
    "\n",
    "Find your desired model from [HuggingFace Model Hub](https://huggingface.co/models) and use the model name in the next command.\n",
    "\n",
    "Supported models:\n",
    "- `LLAMA3 variants`\n",
    "- `Phi-3 variants`\n",
    "- `Gwen-2 variants`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"abacusai/Llama-3-Smaug-8B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Model via HuggingFace\n",
    "To reduce disk usage, download model in `/tmp/model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:37:16.731872Z",
     "iopub.status.busy": "2024-07-24T03:37:16.731531Z",
     "iopub.status.idle": "2024-07-24T03:38:17.881629Z",
     "shell.execute_reply": "2024-07-24T03:38:17.880505Z",
     "shell.execute_reply.started": "2024-07-24T03:37:16.731846Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "g_model_path = Path(\"/tmp/model\")\n",
    "if g_model_path.exists():\n",
    "    shutil.rmtree(g_model_path)\n",
    "g_model_path.mkdir(parents=True)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=repo_id,\n",
    "    ignore_patterns=\"original*\",\n",
    "    local_dir=g_model_path,\n",
    "    token=globals().get(\"HF_TOKEN\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:38:17.884461Z",
     "iopub.status.busy": "2024-07-24T03:38:17.883827Z",
     "iopub.status.idle": "2024-07-24T03:38:19.057551Z",
     "shell.execute_reply": "2024-07-24T03:38:19.056424Z",
     "shell.execute_reply.started": "2024-07-24T03:38:17.884417Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls -l /tmp/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save quantized model\n",
    "Now, load downloaded model on memory with quantization.  \n",
    "This will save storage.\n",
    "\n",
    "\n",
    "Moreover, since the saved model has already been quantized, we do not need `bitsandbytes` package in `main.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:38:19.059437Z",
     "iopub.status.busy": "2024-07-24T03:38:19.059083Z",
     "iopub.status.idle": "2024-07-24T03:38:57.277097Z",
     "shell.execute_reply": "2024-07-24T03:38:57.275976Z",
     "shell.execute_reply.started": "2024-07-24T03:38:19.059408Z"
    }
   },
   "outputs": [],
   "source": [
    "# load model on memory\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "downloaded_model = \"/tmp/model\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    downloaded_model,\n",
    "    quantization_config = bnb_config,\n",
    "    torch_dtype = torch.float16,\n",
    "    device_map = \"auto\",\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(downloaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:38:57.278974Z",
     "iopub.status.busy": "2024-07-24T03:38:57.278493Z",
     "iopub.status.idle": "2024-07-24T03:39:13.179699Z",
     "shell.execute_reply": "2024-07-24T03:39:13.178609Z",
     "shell.execute_reply.started": "2024-07-24T03:38:57.278946Z"
    }
   },
   "outputs": [],
   "source": [
    "# save model in submission directory\n",
    "model.save_pretrained(\"/kaggle/working/submission/model\")\n",
    "tokenizer.save_pretrained(\"/kaggle/working/submission/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:39:13.182210Z",
     "iopub.status.busy": "2024-07-24T03:39:13.181274Z",
     "iopub.status.idle": "2024-07-24T03:39:13.578283Z",
     "shell.execute_reply": "2024-07-24T03:39:13.577210Z",
     "shell.execute_reply.started": "2024-07-24T03:39:13.182169Z"
    }
   },
   "outputs": [],
   "source": [
    "# unload model from memory\n",
    "import gc, torch\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules\n",
    "\n",
    "#### BasicQuestions\n",
    "In this notebook, category will be specified in system prompt after first question.  \n",
    "So, `BasicQuestions` has one question to determine category.\n",
    "\n",
    "#### PlaceQuestions\n",
    "PlaceQuestions has 3 distinct questions to categorize.  \n",
    "Once answer is 'yes', disable hard-coded questions.\n",
    "\n",
    "#### ThingQuestions\n",
    "ThingQuestions has 3 questions.\n",
    "All questions will be asked in order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile submission/questions.py\n",
    "\n",
    "BasicQuestions = [\n",
    "    \"is it a place?\",\n",
    "    # \"is it a thing?\",\n",
    "]\n",
    "\n",
    "PlaceQuestions = [\n",
    "    \"is it a country?\",\n",
    "    \"is it a city?\",\n",
    "    \"is it a natural feature?\",\n",
    "    # \"is it a mountain?\",\n",
    "    # \"is it a river?\",\n",
    "]\n",
    "\n",
    "ThingsQuestions = [\n",
    "    \"is it a living thing?\",   \n",
    "    \"is it edible?\",           \n",
    "    \"is it something that can be held in your hand?\",\n",
    "    \"Does it require electricity to operate?\",\n",
    "    # \"Would the keyword be included in the broad category of [Group]?\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule Based Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile submission/rulebased.py\n",
    "\n",
    "from questions import *\n",
    "\n",
    "\n",
    "class RuleBasedQuestions:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            log (list): A list to store the user's answers.\n",
    "            count (int): The count of questions asked.\n",
    "            enabled (bool): Indicates if all questions have been asked.\n",
    "            category (str): The current category of questions.\n",
    "        \"\"\"\n",
    "        self.log = []\n",
    "        self.count = 0\n",
    "        self.enabled = True\n",
    "        self.category = \"basic\"\n",
    "\n",
    "    def getQuestion(self):\n",
    "        \"\"\"\n",
    "        Returns the next question based on the current state of the game.\n",
    "\n",
    "        Returns:\n",
    "            str: The next question to be asked.\n",
    "        \"\"\"\n",
    "        if self.enabled == False:\n",
    "            return \"No more available questions.\"\n",
    "        \n",
    "        if self.category == \"basic\":\n",
    "            return BasicQuestions[self.count]\n",
    "        elif self.category == \"place\":\n",
    "            return PlaceQuestions[self.count - len(BasicQuestions)]\n",
    "        elif self.category == \"things\":\n",
    "            return ThingsQuestions[self.count - len(BasicQuestions)]\n",
    "\n",
    "    def logAnswer(self, answer):\n",
    "        \"\"\"\n",
    "        Logs the user's answer and updates the category and count based on the answer.\n",
    "\n",
    "        Parameters:\n",
    "        - answer (str): The user's answer, either \"yes\" or \"no\".\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        answer_yes = True\n",
    "        if \"no\" in answer.lower():\n",
    "            answer_yes = False\n",
    "        self.log.append(answer_yes)\n",
    "\n",
    "        self.count += 1\n",
    "\n",
    "        if self.category == \"basic\": \n",
    "            self.category = \"place\" if answer_yes else \"things\"\n",
    "        elif self.category == \"place\":\n",
    "            if answer_yes or self.count == len(BasicQuestions) + len(PlaceQuestions):\n",
    "                self.enabled = False\n",
    "        elif self.category == \"things\":\n",
    "            if self.count == len(BasicQuestions) + len(ThingsQuestions):\n",
    "                self.enabled = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.log = []\n",
    "        self.count = 0\n",
    "        self.enabled = True\n",
    "        self.category = \"basic\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts\n",
    "\n",
    "Prompts are reffered from the [Anthropic Prompt Library](https://docs.anthropic.com/en/prompt-library/library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile submission/prompts.py\n",
    "\n",
    "def asker_prompt(category, obs):\n",
    "    message = []\n",
    "    \n",
    "    # System prompt\n",
    "    ask_prompt = f\"\"\"You are a helpful AI assistant with expertise in playing 20 questions game.\n",
    "Your task is to ask questions to the user to guess the word the user is thinking of.\n",
    "The keyword is of category: \"{obs.category}\"\n",
    "Narrow down the possibilities by asking yes/no questions.\n",
    "Think step by step and try to ask the most informative questions.\n",
    "\\n\"\"\"\n",
    "\n",
    "    message.append({\"role\": \"system\", \"content\": ask_prompt})\n",
    "\n",
    "    # Chat history\n",
    "    for q, a in zip(obs.questions, obs.answers):\n",
    "        message.append({\"role\": \"assistant\", \"content\": q})\n",
    "        message.append({\"role\": \"user\", \"content\": a})\n",
    "\n",
    "    return message\n",
    "\n",
    "\n",
    "def guesser_prompt(category, obs):\n",
    "    message = []\n",
    "    \n",
    "    # System prompt\n",
    "    guess_prompt = f\"\"\"You are a helpful AI assistant with expertise in playing 20 questions game.\n",
    "Your task is to guess the word the user is thinking of.\n",
    "The keyword is of category: \"{category}\"\n",
    "Think step by step.\n",
    "\\n\"\"\"\n",
    "\n",
    "    # Chat history\n",
    "    chat_history = \"\"\n",
    "    for q, a in zip(obs.questions, obs.answers):\n",
    "        chat_history += f\"\"\"Question: {q}\\nAnswer: {a}\\n\"\"\"\n",
    "    prompt = (\n",
    "            guess_prompt + f\"\"\"so far, the current state of the game is as following:\\n{chat_history}\n",
    "        based on the conversation, can you guess the word, please give only the word, no verbosity around\"\"\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    message.append({\"role\": \"system\", \"content\": prompt})\n",
    "    \n",
    "    return message\n",
    "\n",
    "\n",
    "def answerer_prompt(obs):\n",
    "    message = []\n",
    "    \n",
    "    # System prompt\n",
    "    prompt = f\"\"\"You are a helpful AI assistant with expertise in playing 20 questions game.\n",
    "Your task is to answer the questions of the user to help him guess the word you're thinking of.\n",
    "Your answers must be 'yes' or 'no'.\n",
    "The keyword is: \"{obs.keyword}\", it is of category: \"{obs.category}\"\n",
    "Provide accurate answers to help the user to guess the keyword.\n",
    "\"\"\"\n",
    "\n",
    "    message.append({\"role\": \"system\", \"content\": prompt})\n",
    "    \n",
    "    # Chat history\n",
    "    message.append({\"role\": \"user\", \"content\": obs.questions[0]})\n",
    "    \n",
    "    if len(obs.answers)>=1:\n",
    "        for q, a in zip(obs.questions[1:], obs.answers):\n",
    "            message.append({\"role\": \"assistant\", \"content\": a})\n",
    "            message.append({\"role\": \"user\", \"content\": q})\n",
    "    \n",
    "    return message\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "Rule based mode is set to True by default.  \n",
    "If you want to disable rule based mode, \n",
    "uncomment `self.RuleBasedAgent.enabled = False` in `__init__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:59:40.048046Z",
     "iopub.status.busy": "2024-07-24T03:59:40.047609Z",
     "iopub.status.idle": "2024-07-24T03:59:40.058887Z",
     "shell.execute_reply": "2024-07-24T03:59:40.057904Z",
     "shell.execute_reply.started": "2024-07-24T03:59:40.047993Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "# comment magic command before simulation\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from prompts import *\n",
    "from rulebased import RuleBasedQuestions\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    MODEL_PATH = os.path.join(KAGGLE_AGENT_PATH, \"model\")\n",
    "else:\n",
    "    MODEL_PATH = \"/kaggle/working/submission/model\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# specify end-of-turn tokens for your desired model\n",
    "terminators = [tokenizer.eos_token_id]\n",
    "\n",
    "# Additional potential end-of-turn token\n",
    "# llama3, phi3, gwen2 by order\n",
    "potential_terminators = [\"<|eot_id|>\", \"<|end|>\", \"<end_of_turn>\"]\n",
    "\n",
    "for token in potential_terminators:\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    if token_id is not None:\n",
    "        terminators.append(token_id)\n",
    "\n",
    "def generate_response(chat):\n",
    "    inputs = tokenizer.apply_chat_template(chat, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(inputs, max_new_tokens=24, pad_token_id=tokenizer.eos_token_id, eos_token_id=terminators)\n",
    "    response = outputs[0][inputs.shape[-1]:]\n",
    "    out = tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "class Robot:\n",
    "    def __init__(self):\n",
    "        self.RuleBasedAgent = RuleBasedQuestions()\n",
    "        \n",
    "        # To disable the rule-based agent, uncomment the following line\n",
    "        # self.RuleBasedAgent.enabled = False\n",
    "\n",
    "    def on(self, mode, obs):\n",
    "        assert mode in [\n",
    "            \"asking\", \"guessing\", \"answering\",\n",
    "        ], \"mode can only take one of these values: asking, answering, guessing\"\n",
    "        if mode == \"asking\":\n",
    "            # launch the asker role\n",
    "            output = self.asker(obs)\n",
    "        if mode == \"answering\":\n",
    "            # launch the answerer role\n",
    "            output = self.answerer(obs)\n",
    "            if \"yes\" in output.lower():\n",
    "                output = \"yes\"\n",
    "            elif \"no\" in output.lower():\n",
    "                output = \"no\"\n",
    "            if \"yes\" not in output.lower() and \"no\" not in output.lower():\n",
    "                output = \"yes\"\n",
    "        if mode == \"guessing\":\n",
    "            # launch the guesser role\n",
    "            output = self.guesser(obs)\n",
    "        return output\n",
    "\n",
    "    def asker(self, obs):\n",
    "        # if the rule-based agent is enabled, use it to ask questions\n",
    "        if self.RuleBasedAgent.enabled:\n",
    "            output = self.RuleBasedAgent.getQuestion()\n",
    "            return output\n",
    "        \n",
    "        input = asker_prompt(self.RuleBasedAgent.category, obs)\n",
    "        output = generate_response(input)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def guesser(self, obs):\n",
    "        # if the rule-based agent is enabled, log the answer \n",
    "        if self.RuleBasedAgent.enabled:\n",
    "            self.RuleBasedAgent.logAnswer(obs.answers[-1])\n",
    "        \n",
    "        input = guesser_prompt(self.RuleBasedAgent.category, obs)\n",
    "        output = generate_response(input)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def answerer(self, obs):\n",
    "        input = answerer_prompt(obs)\n",
    "        output = generate_response(input)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "robot = Robot()\n",
    "\n",
    "\n",
    "def agent(obs, cfg):\n",
    "\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = robot.on(mode=\"asking\", obs=obs)\n",
    "\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = robot.on(mode=\"guessing\", obs=obs)\n",
    "\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = robot.on(mode=\"answering\", obs=obs)\n",
    "\n",
    "    if response == None or len(response) <= 1:\n",
    "        response = \"yes\"\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:54:18.800659Z",
     "iopub.status.busy": "2024-07-24T03:54:18.800218Z",
     "iopub.status.idle": "2024-07-24T03:54:35.726272Z",
     "shell.execute_reply": "2024-07-24T03:54:35.725098Z",
     "shell.execute_reply.started": "2024-07-24T03:54:18.800628Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run game, you need to specify agent. Before execute next cell, excute main.py cell above with commenting `%%writefile -a submission/main.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:54:35.729109Z",
     "iopub.status.busy": "2024-07-24T03:54:35.728748Z",
     "iopub.status.idle": "2024-07-24T03:57:57.075552Z",
     "shell.execute_reply": "2024-07-24T03:57:57.074453Z",
     "shell.execute_reply.started": "2024-07-24T03:54:35.729080Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from kaggle_environments import make\n",
    "# env = make(\"llm_20_questions\", debug=True)\n",
    "# game_output = env.run(agents=[agent, agent, agent, agent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:57:57.077693Z",
     "iopub.status.busy": "2024-07-24T03:57:57.076802Z",
     "iopub.status.idle": "2024-07-24T03:57:57.152952Z",
     "shell.execute_reply": "2024-07-24T03:57:57.151984Z",
     "shell.execute_reply.started": "2024-07-24T03:57:57.077660Z"
    }
   },
   "outputs": [],
   "source": [
    "# env.render(mode=\"ipython\", width=600, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:39:20.205260Z",
     "iopub.status.busy": "2024-07-24T03:39:20.204918Z",
     "iopub.status.idle": "2024-07-24T03:39:31.595446Z",
     "shell.execute_reply": "2024-07-24T03:39:31.594100Z",
     "shell.execute_reply.started": "2024-07-24T03:39:20.205232Z"
    }
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T03:59:51.292752Z",
     "iopub.status.busy": "2024-07-24T03:59:51.292086Z",
     "iopub.status.idle": "2024-07-24T04:01:41.448825Z",
     "shell.execute_reply": "2024-07-24T04:01:41.447529Z",
     "shell.execute_reply.started": "2024-07-24T03:59:51.292718Z"
    }
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
