# 📚 from_paper_to_code

Welcome to **from_paper_to_code** – a personal lab for reimplementing and experimenting with machine learning and deep learning research papers.  
This repository contains implementations of various academic papers, reproduced for learning, analysis, and benchmarking.

---

## 🎯 Purpose

- To better understand the inner workings of published ML/DL research
- To explore reproducibility and practical performance of theoretical models
- To build a hands-on portfolio of SOTA models from scratch or with minimal guidance

---

## 🗂 Directory Structure

Each folder represents a reimplementation of a specific paper.  
The folder names typically follow this format:  
`<paper-name>-<year>` or `<key-concept>`.
paper-lab/
├── transformer_2017/         # Attention Is All You Need
├── graphmae_2022/            # GraphMAE: Self-Supervised Masked Autoencoders for Graphs
├── efficientnet/             # EfficientNet: Rethinking Model Scaling for CNNs
└── README.md

Each subdirectory contains:
- 📄 `README.md` describing the paper summary, objective, and implementation details
- 🧠 Core model code (e.g. `model.py`, `train.py`)
- 📊 Sample results, logs, and visualizations (if applicable)
- 📁 Possibly pre-trained weights or checkpoints

---

## 📌 Completed Papers

| Paper | Year | Model | Status |
|-------|------|--------|--------|
| Attention is All You Need | 2017 | Transformer | ✅ Completed |
| GraphMAE | 2022 | Graph Autoencoder | 🔧 In Progress |
| EfficientNet | 2019 | CNN Scaling | ✅ Completed |
<!-- Add your own entries here -->

---

## 🚧 To-Do

- [ ] Add evaluation scripts for each model
- [ ] Compare with official benchmarks
- [ ] Refactor common utility functions

---

## 🤝 Contributions

This project is for personal study, but suggestions and feedback are welcome!  
If you're interested in collaborating on a specific paper, feel free to reach out or fork and open a PR.

---

## 📬 Contact

You can find me via GitHub or email for discussion, collaboration, or feedback.

---

## 📜 License

This project is for educational purposes only. If original paper authors request removal of any content, it will be respectfully done.